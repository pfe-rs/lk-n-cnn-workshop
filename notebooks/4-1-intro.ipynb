{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vežbe iz konvolucionih neuralnih mreža - Uvod\n",
    "## Letnji kamp za nove polaznike\n",
    "\n",
    "Cilj ove vežbe je da pruži prvi kontakt sa konvolucionim neuralnim mrežama, ali i da vas upozna sa bibliotekom [PyTorch](https://pytorch.org).\n",
    "\n",
    "PyTorch je biblioteka koja automatizuje jako puno procesa koji bi se inače morali ručno pisati. Glavni od njih se tiče __komputacionog grafa__ i algoritma __propagacije greške unazad__. Naime, PyTorch implementira sve savremene slojeve neuralnih mreža, i nudi mogućnost njihovog povezivanja u neuralne mreže. Posledično, PyTorch modeli automatski održavaju komputacioni graf i računaju sve potrebne gradijente!\n",
    "\n",
    "Ova sveska je okvirno podeljena na sledeće celine:\n",
    "\n",
    "* Baratanje podacima\n",
    "* Kreiranje neuralne mreže\n",
    "* Treniranje neuralne mreže\n",
    "\n",
    "## Baratanje podacima\n",
    "\n",
    "Upoznavanje sa PyTorch-om i konvolucionim neuralnim mrežama ćemo odraditi nad zadatkom klasifikacije odeće iz dataseta `FashionMNIST`. Iako se informacije o ovom skupu podataka mogu naći vrlo jednostavno na internetu, počnimo od toga da ne znamo ništa o njemu i da moramo da ga malo istražimo.\n",
    "\n",
    "Za početak učitajmo potrebne biblioteke:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from utils import sample_to_pil_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U ćeliji ispod učitavamo `FashionMNIST` skup podataka. [PyTorch Datsets](https://pytorch.org/vision/stable/datasets.html) je bogata biblioteka sa puno skupova podataka koji se koriste za različite namene. Prednost korišćenja tih skupova podataka je što se učitavaju u doslovno četiri linije koda, kao što je prikazano ispod. Mnogi skupovi podataka, međutim, imaju određene specifičnosti i ne podpadaju pod datu biblioteku, kao što ćete videti u narednim notebook-ovima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\"../dataset/data\", download=True, train=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "test_set = torchvision.datasets.FashionMNIST(\"../dataset/data\", download=True, train=False, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, shuffle=False, num_workers=2, batch_size=100)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, shuffle=False, num_workers=2, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre svega pogledajmo kako izgleda naša baza slika. Slobodno pokrenite ćeliju ispod više puta kako biste videli različite primere / klase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_sample(dataloader):\n",
    "    batched_images, batched_classes = next(iter(dataloader))\n",
    "    random_index = np.random.randint(0, batched_images.shape[0])\n",
    "    random_image, random_class = batched_images[random_index], batched_classes[random_index]\n",
    "    return random_image, random_class\n",
    "\n",
    "random_image, random_label = get_random_sample(dataloader=test_loader)\n",
    "image = sample_to_pil_image(sample=random_image, image_shape=random_image.shape[1:])\n",
    "print(f\"Sample image for class {test_set.classes[random_label]} is shown below:\")\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hajde kvantitativno da ispitamo dataset. Kod ispod najpre proverava koliko imamo primera u skupu za obučavanje i skupu za testiranje, a posle proveri koliko imamo primera po klasi. Skpovi podataka koji imaju otprilike jednak broj primera za svaku klasu nazivaju se __balansiranim__ skupovima podataka i sa njima je mnogo lakše raditi:\n",
    "\n",
    "1. __tačnost__ kao metrika ima smisla. Posledično ne moramo uvoditi komplikovanije metrike\n",
    "2. Ne moramo posebno da pazimo da raspodela klasa u skupu za obučavanje i testiranje budu iste (nasumično uzorkovanje će to obezbediti)\n",
    "3. Ne moramo uvoditi augmentacije podataka, veštački balansirati skup podataka, i sl.\n",
    "4. Mreža prirodno ima priliku da se obuči podjednako dobro za svaku klasu (naravno neke mogu biti teže, ali makar ih ima u jednakoj meri)\n",
    "\n",
    "Kao što vidimo, na svu sreću, `FashionMNIST` je balansiran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "print(f\"Train set consists of {len(train_set)} categorized into {len(train_set.classes)} classes.\")\n",
    "print(f\"Test set consists of {len(test_set)} categorized into {len(test_set.classes)} classes.\")\n",
    "print(f\"Image dimension is {random_image.shape}\")\n",
    "\n",
    "label_dict = defaultdict(int)\n",
    "for _, label in train_set:\n",
    "    label_name = train_set.classes[label]\n",
    "    label_dict[label_name] += 1\n",
    "print(label_dict)\n",
    "\n",
    "label_dict = defaultdict(int)\n",
    "for _, label in test_set:\n",
    "    label_name = test_set.classes[label]\n",
    "    label_dict[label_name] += 1\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kreiranje mreže\n",
    "\n",
    "Na vežbama iz neuralnih mreža smo morali za svaki sloj ručno da implementiramo:\n",
    "* Prolazak podataka unapred kroz `forward` metodu\n",
    "* Propagaciju greške unazad kroz `backward` metodu\n",
    "\n",
    "PyTorch nudi dve opcije:\n",
    "* Korišćenje već gotovog sloja kome su obe metode implementirane\n",
    "* Definisanje svog sloja __kome je potrebno implementirati samo `forward` metodu!__ Na osnovu operacija u toj metodi, i po pravilu propagacije greške unazad, PyTorch sam računa gradijente\n",
    "\n",
    "U ćeliji ispod koristimo prvi pristup, gde su nam svi slojevi i operacije već implementirane. Konkretno, prvi sloj mreže čine:\n",
    "\n",
    "1. Krećemo od operacije konvolucije `Conv2d` sa brojem neurona 32, veličinom filtra 3 i popunjavanjem ivica (__padding__) 1.\n",
    "2. Normalizujemo izlaz prethodne operacije `BatchNorm2d` operacijom\n",
    "3. Primenimo aktivacionu funkciju `ReLU`\n",
    "4. Primenimo `MaxPool2d` sa veličinom filtra 2 i korakom 2\n",
    "\n",
    "Možda je malo konfuzno, ali većina od prethodno navedenih operacija se sama po sebi može nazvati slojem, a možemo ih grupisati i celu sekvencu operacija takođe nazvati slojem! Konkretno, PyTorch koristi klasu [Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) da predstavi sloj, pri čemu `Module` može sadržati druge `Module`-e.\n",
    "\n",
    "#### Pitanja\n",
    "\n",
    "1. Zašto je `in_channels=1`?\n",
    "1. Zašto nam je `out_features=10` u poslednjem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_architecture = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    \n",
    "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Flatten(),\n",
    "\n",
    "    nn.Linear(in_features=64*6*6, out_features=600),\n",
    "    nn.Dropout(0.25),\n",
    "    nn.Linear(in_features=600, out_features=120),\n",
    "    nn.Linear(in_features=120, out_features=10)\n",
    ")\n",
    "neural_net = mnist_architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konvolucione neuralne mreže znaju biti poprilično duboke i imati jako puno slojeva. Treniranje takvih mreža je najčešće vrlo komputaciono zahtevno, te je bitno na kom uređaju se treniraju:\n",
    "\n",
    "* CPU (procesor): Manji potencijal za paralelizacijom, ali brza jezgra.\n",
    "* GPU (grafička kartica): Veliki potencijal za paralelizacijom, ali spora jezgra.\n",
    "\n",
    "Zbog prirode operacija neuralnih mreža koje se mogu paralelizovati, najviše im odgovara treniranje na grafičkoj kartici. PyTorch nudi vrlo jednostavno prebacivanje podataka sa jednog resursa na drugi.\n",
    "\n",
    "Ćelija ispod napre odredi koji tip resursa nam je dostupan i odabere GPU ako postoji, i onda prebaci model na odgovarajući resurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "neural_net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treniranje neuralne mreže\n",
    "\n",
    "U ćeliji ispod definišemo:\n",
    "1. __funkciju gubitka__ koju želimo da koristimo, koja je ista kao i na prethodnoj vežbi. \n",
    "2. stopu učenja\n",
    "3. __optimizator__: Ovo je novi koncept. Na prošlim vežbama je bilo reči o algoritmu __gradijentnog spusta__. Postoje tri tipa gradijentnog spusta:\n",
    "    * __stohastički gradijentni spust__: Osvežavanje težina mreže se radi nakon svakog primera koji prođe kroz mrežu, odnosno svaki primer zasebno utiče na težine.\n",
    "    * __šaržni gradijentni spust__: Osvežavanje težina mreže se radi tek nakon što svi primeri prođu kroz mrežu, odnosno gradijenti greške za svaki od njih se najpre usrednje.\n",
    "    * __mini-šaržni gradijentni spust__: Između stohastičkog i šaržnog, primeri prolaze kroz mrežu u grupama fiksne veličine.\n",
    "    \n",
    "    Svaki od ovih tipova gradijentnog spusta, međutim, se može koristiti sa još puno dodatnih parametara koji mogu značajno uticati na brzinu konvergencije modela. Zbog toga PyTorch uvodi klasu `Optimizer` koja enkapsulira parametre i tip gradijentnog spusta.\n",
    "\n",
    "#### Pitanja\n",
    "\n",
    "1. Koji tip gradijentnog spusta deluje da se koristi ispod? Ako pogledamo deo koda gde kreiramo `DataLoader`, da li deluje da koristimo mini-šaržni gradijentni spust?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.SGD(neural_net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kod ispod trenira neuralnu mrežu. Komentari u kodu objašnjavaju i specifičnosti PyTorch-a, ali i povezuju treniranje sa prethodnim vežbama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_accuracy, plot_curve, keep_store_dict, store_dict_to_disk\n",
    "\n",
    "def test(model, test_loader):\n",
    "    # Put the model in evaluation mode. \n",
    "    # Tells the model not to compute gradients. Increases inference speed.\n",
    "    model.eval()\n",
    "    test_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_num, (x, y) in enumerate(test_loader, 0):\n",
    "            # Put the data to the appropriate device.\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            # Do inference. Forwad pass with the model.\n",
    "            y_hat = model(x)\n",
    "            test_acc += get_accuracy(y_hat, y)\n",
    "    return test_acc / batch_num\n",
    "\n",
    "def train(model, num_epochs, train_loader, store_dict, test_loader=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        # Keep track of accuracy and loss accross batches\n",
    "        train_running_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "\n",
    "        # Put the model in training mode. \n",
    "        # Tells the model to keep track of gradients. Reduces inference speed.\n",
    "        model = model.train()\n",
    "\n",
    "        # A single training step\n",
    "        for batch_num, (x, y) in enumerate(train_loader):\n",
    "            \n",
    "            # Put the data to the appropriate device.\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Do inference. Forwad pass with the model.\n",
    "            y_hat = model(x)\n",
    "            \n",
    "            # Calculate the loss. Recall the computation graph.\n",
    "            loss = loss_function(y_hat, y)\n",
    "            \n",
    "            # Do backpropagation and gradient descent step. ####\n",
    "            # 1. Reset remembered gradients. (Optimizers remember gradients from previous \n",
    "            # iterations, we do not want these to affect the current step)\n",
    "            optimizer.zero_grad()\n",
    "            # Do backpropagation algorithm and calculate all relevant gradients.\n",
    "            loss.backward()\n",
    "            # Update model parameters (weights and biases) w.r.t. computed gradients.\n",
    "            optimizer.step()\n",
    "\n",
    "            train_running_loss += loss.detach().item()\n",
    "            train_acc += get_accuracy(y_hat=y_hat, y=y)\n",
    "\n",
    "        \n",
    "        epoch_loss = train_running_loss / batch_num\n",
    "        epoch_acc = train_acc / batch_num\n",
    "        \n",
    "        store_dict = keep_store_dict(curve=epoch_loss, label='train_loss', store_dict=store_dict)\n",
    "        store_dict = keep_store_dict(curve=epoch_acc, label='train_acc', store_dict=store_dict)\n",
    "        print('Epoch: %d | Loss: %.4f | Train Accuracy: %.2f' \\\n",
    "            %(epoch, epoch_loss, epoch_acc))\n",
    "\n",
    "        if test_loader is not None:\n",
    "            test_acc = test(model=model, test_loader=test_loader)\n",
    "            store_dict = keep_store_dict(curve=test_acc, label='test_acc', store_dict=store_dict)\n",
    "        \n",
    "    return store_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ćelija ispod pušta trening i čuva rezultate na disk, na putanju `results/mnist.json`.\n",
    "\n",
    "__Napomena__: Ćelija se može puštati više puta u kom slučaju će trening modela biti nastavljen. Rezultati koji se čuvaju na disku će biti samo nadovezani na već postojeće. Ukoliko želite da trenirate model od nule, iznova, potrebno je pustiti sve ćelije od definicije modela pa na dalje, i __ručno izbrisati `results/mnist.json`__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_result_path = 'results/mnist.json'\n",
    "store_dict = train(model=neural_net, num_epochs=NUM_EPOCHS, train_loader=train_loader, test_loader=test_loader, store_dict=None)\n",
    "store_dict_to_disk(file_path=mnist_result_path, store_dict=store_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rezultati su sačuvani na putanji `results/mnist.json`. Rezultate sada možemo plotovati puštanjem ćelije ispod.\n",
    "\n",
    "### Pitanja\n",
    "\n",
    "1. Da li je model dovoljno obučen?\n",
    "2. Da li je model preobučen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "from utils import load_dict_from_disk\n",
    "\n",
    "store_dict = load_dict_from_disk(file_path=mnist_result_path)\n",
    "\n",
    "# We plot accuracy here. We could also have plotted the loss.\n",
    "fig = plot_curve(curves=[store_dict['train_acc'], store_dict['test_acc']], labels=['train_acc', 'test_acc'], plot_name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konačno, u sledećoj ćelji možemo videti kako model zaključuje na nasumičnim primerima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we have not already loaded sample image in the intro\n",
    "if random_image is None:\n",
    "    random_image, random_label = get_random_sample(dataloader=test_loader)\n",
    "\n",
    "image = sample_to_pil_image(sample=random_image, image_shape=random_image.shape[1:])\n",
    "\n",
    "x = torch.unsqueeze(random_image.to(device), 0)\n",
    "y_hat = np.argmax(neural_net(x).detach().cpu().numpy())\n",
    "y_hat = test_set.classes[y_hat]\n",
    "y = test_set.classes[random_label]\n",
    "print(y_hat)\n",
    "\n",
    "print(f\"Sample image for class {y} is shown below. Model prediction is {y_hat}.\")\n",
    "display(image)\n",
    "\n",
    "random_image, random_label = get_random_sample(dataloader=test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "ea0c376a256e2d4f1d520d02610d07fa9317181a0f87e08d36ab97c29d79bdd2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
